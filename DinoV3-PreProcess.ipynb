{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "import pickle\n",
    "import os\n",
    "import urllib\n",
    "from functools import reduce\n",
    "from collections import deque\n",
    "import gc\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import convolve\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "# import all utils\n",
    "from utils import data_directory, \\\n",
    "                dinov3_repo_dir, \\\n",
    "                dinov3_vitb16_weight_raw, \\\n",
    "                dinov3_vith16_weight_raw, \\\n",
    "                dinov3_vit7B16_weight_raw, \\\n",
    "                dinov3_vits16_weight_raw\n",
    "                \n",
    "sys.path.insert(0, os.path.join(dinov3_repo_dir))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Image Setting\n",
    "PATCH_SIZE = 16\n",
    "DEFAULT_IMAGE_SIZE = 512 # Should be multiple of PATCH_SIZE\n",
    "MASK_THRESHOLD = 0\n",
    "\n",
    "# Batch Settings\n",
    "BATCH_SIZE = 8\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Available DINOv3 models:\n",
    "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
    "MODEL_DINOV3_VITSP = \"dinov3_vits16plus\"\n",
    "MODEL_DINOV3_VITB = \"dinov3_vitb16\"\n",
    "MODEL_DINOV3_VITL = \"dinov3_vitl16\"\n",
    "MODEL_DINOV3_VITHP = \"dinov3_vith16plus\"\n",
    "MODEL_DINOV3_VIT7B = \"dinov3_vit7b16\"\n",
    "\n",
    "MODEL_TO_NUM_LAYERS = {\n",
    "    MODEL_DINOV3_VITS: 12,\n",
    "    MODEL_DINOV3_VITSP: 12,\n",
    "    MODEL_DINOV3_VITB: 12,\n",
    "    MODEL_DINOV3_VITL: 24,\n",
    "    MODEL_DINOV3_VITHP: 32,\n",
    "    MODEL_DINOV3_VIT7B: 40,\n",
    "}\n",
    "\n",
    "MODEL_TO_EMBED_DIM = {\n",
    "    MODEL_DINOV3_VITS: 384,\n",
    "    MODEL_DINOV3_VITSP: 384,  # ViT-Small+\n",
    "    MODEL_DINOV3_VITB: 768,\n",
    "    MODEL_DINOV3_VITL: 1024,\n",
    "    MODEL_DINOV3_VITHP: 1536, # ViT-Huge+\n",
    "    MODEL_DINOV3_VIT7B: 4096,\n",
    "}\n",
    "\n",
    "MODEL_TO_WEIGHT_FILE = {\n",
    "    MODEL_DINOV3_VITS: dinov3_vits16_weight_raw,\n",
    "    MODEL_DINOV3_VITB: dinov3_vitb16_weight_raw,\n",
    "    MODEL_DINOV3_VITHP: dinov3_vith16_weight_raw, # ViT-Huge+\n",
    "}\n",
    "\n",
    "def count_components_floodfill(row:dict, connectivity: int = 8):\n",
    "    \"\"\"\n",
    "    Count connected True components in a 2D boolean numpy mask using flood-fill (iterative).\n",
    "    Args:\n",
    "        mask: 2D boolean numpy array\n",
    "        connectivity: 4 or 8 (neighbour connectivity)\n",
    "    Returns:\n",
    "        n_components: int\n",
    "        sizes: list of int (size of each component)\n",
    "        labels: 2D int32 array same shape as mask where 0 = background, 1..n = component ids\n",
    "    \"\"\"\n",
    "    mask = row['bool_mask']\n",
    "    \n",
    "    assert mask.ndim == 2, \"mask must be 2D\"\n",
    "    assert connectivity in (4, 8)\n",
    "    H, W = mask.shape\n",
    "    visited = np.zeros((H, W), dtype=bool)\n",
    "    # labels = np.zeros((H, W), dtype=np.int32)\n",
    "    neighbors4 = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    neighbors8 = neighbors4 + [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "    neighbors = neighbors8 if connectivity == 8 else neighbors4\n",
    "\n",
    "    comp_id = 0\n",
    "    # sizes = []\n",
    "\n",
    "    for y in range(H):\n",
    "        for x in range(W):\n",
    "            if mask[y, x] and not visited[y, x]:\n",
    "                comp_id += 1\n",
    "                q = deque()\n",
    "                q.append((y, x))\n",
    "                visited[y, x] = True\n",
    "                # labels[y, x] = comp_id\n",
    "                # size = 0\n",
    "                while q:\n",
    "                    cy, cx = q.popleft()\n",
    "                    # size += 1\n",
    "                    for dy, dx in neighbors:\n",
    "                        ny, nx = cy + dy, cx + dx\n",
    "                        if 0 <= ny < H and 0 <= nx < W and not visited[ny, nx] and mask[ny, nx]:\n",
    "                            visited[ny, nx] = True\n",
    "                            # labels[ny, nx] = comp_id\n",
    "                            q.append((ny, nx))\n",
    "                # sizes.append(size)\n",
    "\n",
    "    return {\"name\":row[\"name\"], \"index\":row[\"index\"], \"bool_mask\":mask, \"n_comps\":comp_id} #, sizes, labels\n",
    "\n",
    "def resize_image_to_fit_patch(\n",
    "    image: Image,\n",
    "    image_size: int = DEFAULT_IMAGE_SIZE,\n",
    "    patch_size: int = PATCH_SIZE,\n",
    ") -> Image:\n",
    "    w, h = image.size\n",
    "    w_hat = h_hat = PATCH_SIZE * (image_size // PATCH_SIZE)\n",
    "    resized_img = image.resize((w_hat, h_hat), \\\n",
    "                    resample=Image.Resampling.LANCZOS)\n",
    "    return resized_img\n",
    "\n",
    "def resized_image_to_mask(image_resized, mask_threshold: int = MASK_THRESHOLD):\n",
    "    image_array = np.array(image_resized)\n",
    "    mask = image_array > mask_threshold\n",
    "    return mask\n",
    "\n",
    "def mask_to_resized_image(mask):\n",
    "    image = Image.fromarray((mask * 255).astype(np.uint8), mode='L')\n",
    "    image_resized = resize_image_to_fit_patch(image)\n",
    "    return image_resized\n",
    "\n",
    "def resize_mask_to_fit_patch(\n",
    "    mask: np.ndarray,\n",
    "    mask_threshold: int = MASK_THRESHOLD,\n",
    ") -> np.ndarray:\n",
    "    resized_image = mask_to_resized_image(mask)\n",
    "    return resized_image_to_mask(resized_image)\n",
    "\n",
    "def pixel_to_patch_coords(y_pixel, x_pixel, patch_size=PATCH_SIZE):\n",
    "    y_coord = y_pixel // patch_size\n",
    "    x_coord = x_pixel // patch_size\n",
    "    return y_coord, x_coord\n",
    "\n",
    "# Convert auth_mask to patch coordinates and mask the similarity map\n",
    "def pixel_mask_to_patch_float(pixel_mask, patch_size=PATCH_SIZE):\n",
    "    \"\"\"Convert pixel-level mask to patch-level mask\"\"\"\n",
    "    H_pixels, W_pixels = pixel_mask.shape\n",
    "    H_patches = H_pixels // patch_size\n",
    "    W_patches = W_pixels // patch_size\n",
    "    \n",
    "    patch_mask = np.zeros((H_patches, W_patches), dtype=float)\n",
    "    \n",
    "    for i in range(H_patches):\n",
    "        for j in range(W_patches):\n",
    "            # Check if any pixel in this patch is True\n",
    "            patch_region = pixel_mask[i*patch_size:(i+1)*patch_size, \n",
    "                                     j*patch_size:(j+1)*patch_size]\n",
    "            patch_mask[i, j] = np.sum(patch_region) / (patch_size * patch_size)\n",
    "    \n",
    "    return patch_mask\n",
    "\n",
    "def load_10_mask_entities(name=None, idxs=None):\n",
    "    all_mask_entities = []\n",
    "    if name is not None and idxs is not None:\n",
    "        for idx in idxs:\n",
    "            file = os.path.join(\"mask_entities\", f\"{name}_{idx}.pkl\")\n",
    "            all_mask_entities.append(pickle.load(open(file, 'rb')))\n",
    "        return all_mask_entities\n",
    "    files = glob(os.path.join(\"mask_entities\", '*.pkl'))[:10]\n",
    "    all_mask_entities = []\n",
    "    for file in files:\n",
    "        mask_entities = pickle.load(open(file, 'rb'))\n",
    "        all_mask_entities.append(mask_entities)\n",
    "    return all_mask_entities\n",
    "\n",
    "def auth_and_forged_sim_mat(auth_mask, forged_mask):\n",
    "        \"\"\"\n",
    "        Calculate similarity matrix between authentic and forged patches using matrix multiplication\n",
    "            \n",
    "        Returns:\n",
    "            similarity_matrix: Tensor of shape [H1, W1, H2, W2]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get dimensions\n",
    "        F1, W1 = auth_mask.shape\n",
    "        F2, W2 = forged_mask.shape\n",
    "            \n",
    "        similarity_matrix = torch.matmul(auth_mask, forged_mask.transpose(-1, -2))\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "def get_batches(batch_size:int = BATCH_SIZE):\n",
    "    \n",
    "    # Files\n",
    "    files = glob(os.path.join(\"mask_entities\", '*.pkl'))\n",
    "    \n",
    "    # Names\n",
    "    names = sorted(list({x.split(os.sep)[-1].split('_')[0] for x in files}))\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(names):\n",
    "        forged_img_tensors = []\n",
    "        all_sims = []\n",
    "        ids = []\n",
    "        \n",
    "        batch_files = names[i:i+batch_size]\n",
    "        \n",
    "        for name in batch_files:\n",
    "            \n",
    "            ids.append(name)\n",
    "            \n",
    "            name_files = glob(os.path.join(\"mask_entities\", f'{name}_*.pkl'))\n",
    "            \n",
    "            forged_image_path = os.path.join(forged_folder, name + '.png')\n",
    "            forged_img = Image.open(forged_image_path).convert('RGB')\n",
    "            forged_img_resized = resize_image_to_fit_patch(forged_img)\n",
    "            forged_img_tensor = TF.to_tensor(forged_img_resized)\n",
    "            forged_image_normalized_tensor = TF.normalize(forged_img_tensor, \\\n",
    "                                        mean=IMAGENET_MEAN, \\\n",
    "                                        std=IMAGENET_STD)\n",
    "            forged_img_tensors.append(forged_image_normalized_tensor)\n",
    "            \n",
    "            sims = []\n",
    "            for file in name_files:\n",
    "                me = pickle.load(open(file, 'rb'))\n",
    "                \n",
    "                auth_mask = resize_mask_to_fit_patch(me[\"auth_mask\"])\n",
    "                auth_mask = pixel_mask_to_patch_float(auth_mask)\n",
    "                auth_mask_tensor = torch.from_numpy(auth_mask) \\\n",
    "                                        .unsqueeze(0) \\\n",
    "                                        .permute(1, 2, 0)\n",
    "                auth_mask_tensor = auth_mask_tensor.reshape(-1, auth_mask_tensor.shape[-1])\n",
    "                \n",
    "                forged_mask = resize_mask_to_fit_patch(me[\"forged_mask\"])\n",
    "                forged_mask = pixel_mask_to_patch_float(forged_mask)\n",
    "                forged_mask_tensor = torch.from_numpy(forged_mask) \\\n",
    "                                        .unsqueeze(0) \\\n",
    "                                        .permute(1, 2, 0)\n",
    "                forged_mask_tensor = forged_mask_tensor.reshape(-1, forged_mask_tensor.shape[-1])\n",
    "                \n",
    "                # sim = auth_and_forged_sim_mat(auth_mask_tensor, forged_mask_tensor) > 0\n",
    "                # sim = auth_and_forged_sim_mat(forged_mask_tensor, auth_mask_tensor) > 0\n",
    "                sim = auth_and_forged_sim_mat(forged_mask_tensor, forged_mask_tensor) > 0\n",
    "                sims.append(sim)\n",
    "            \n",
    "            # Combine sims by logical or\n",
    "            combined_sim = reduce(lambda x, y: torch.logical_or(x,y), sims).float()\n",
    "            # combined_sim[combined_sim==0] = -1.0\n",
    "            all_sims.append(combined_sim.float())\n",
    "            \n",
    "        forged_img_tensors = torch.stack(forged_img_tensors, dim=0)\n",
    "        all_sims_tensor = torch.stack(all_sims, dim=0)\n",
    "        \n",
    "        # print(forged_img_tensors.shape, all_sims_tensor.shape)\n",
    "        \n",
    "        yield forged_img_tensors, all_sims_tensor, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e06ff9",
   "metadata": {},
   "source": [
    "### Extract Forged and Authentic Masks Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2717f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(mask_file):\n",
    "    ret = []\n",
    "    \n",
    "    mask_name = os.path.basename(mask_file).replace('.npy', '')\n",
    "    \n",
    "    if os.path.exists(os.path.join(forged_folder, mask_name+'.png')) is False:\n",
    "        return ret\n",
    "\n",
    "    if os.path.exists(os.path.join(authentic_folder, mask_name+'.png')) is False:\n",
    "        return ret\n",
    "    \n",
    "    masked_image_path = os.path.join(mask_folder, mask_name+'.npy')\n",
    "    masked_imgs = np.load(masked_image_path)\n",
    "    masked_imgs_shape = masked_imgs.shape\n",
    "    \n",
    "    for i in range(masked_imgs_shape[0]):\n",
    "    \n",
    "        # get the i-th mask\n",
    "        masked_img = masked_imgs[i]\n",
    "\n",
    "        # Convert masked_img to boolean\n",
    "        masked_img_boolean = masked_img > MASK_THRESHOLD\n",
    "        \n",
    "        ret.append({\"name\":mask_name, \"index\":i, \"bool_mask\":masked_img_boolean})\n",
    "    \n",
    "    return ret\n",
    "\n",
    "mask_files = glob(os.path.join(mask_folder, '*.npy'))\n",
    "all_masks = Parallel(n_jobs=-1)(delayed(get_masks)(mask_file) for mask_file in tqdm(mask_files))\n",
    "all_masks = [item for sublist in all_masks for item in sublist]\n",
    "pickle.dump(all_masks, open('masks.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad78c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks = pickle.load(open('masks.pkl', 'rb'))\n",
    "component_counts = Parallel(n_jobs=-1)(delayed(count_components_floodfill)(row) for row in tqdm(all_masks))\n",
    "pickle.dump(component_counts, open('component_counts.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eb625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each set of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def smooth_boolean_mask(mask, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Smooths a boolean pixel mask based on a majority vote of neighboring pixels.\n",
    "    Kernel/accumulators use int32 to avoid overflow for large kernels.\n",
    "    \"\"\"\n",
    "    if kernel_size % 2 == 0 or kernel_size < 3:\n",
    "        raise ValueError(\"kernel_size must be an odd integer >= 3\")\n",
    "\n",
    "    # Use int32 to avoid overflow when summing large neighborhoods\n",
    "    int_mask = mask.astype(np.int32)\n",
    "\n",
    "    kernel = np.ones((kernel_size, kernel_size), dtype=np.int32)\n",
    "\n",
    "    neighbor_sum = convolve(int_mask, kernel, mode='constant', cval=0)\n",
    "\n",
    "    # majority threshold excluding center? here we keep original behavior:\n",
    "    majority_threshold = (kernel_size * kernel_size) / 2\n",
    "\n",
    "    new_mask = neighbor_sum > majority_threshold\n",
    "\n",
    "    return new_mask.astype(bool)\n",
    "\n",
    "def smooth_boolean_mask2(mask, kernel_size=3):\n",
    "    \"\"\"\n",
    "    Smooth boolean mask: a pixel becomes False only if a strict majority of its neighbors\n",
    "    (excluding itself) are False; otherwise it keeps its original value.\n",
    "\n",
    "    Uses int32 accumulator to prevent overflow when kernel_size grows.\n",
    "    \"\"\"\n",
    "    if kernel_size % 2 == 0 or kernel_size < 3:\n",
    "        raise ValueError(\"kernel_size must be an odd integer >= 3\")\n",
    "\n",
    "    # Use int32 to avoid overflow\n",
    "    int_mask = mask.astype(np.int32)\n",
    "\n",
    "    kernel = np.ones((kernel_size, kernel_size), dtype=np.int32)\n",
    "\n",
    "    # Convolve to get sum including center\n",
    "    neighbor_sum_incl = convolve(int_mask, kernel, mode='constant', cval=0)\n",
    "\n",
    "    # Sum of neighbors only (exclude the center pixel)\n",
    "    neighbors_sum = neighbor_sum_incl - int_mask\n",
    "\n",
    "    # Number of neighbors (excluding center)\n",
    "    n_neighbors = kernel_size * kernel_size - 1\n",
    "\n",
    "    # Count of False neighbors\n",
    "    false_neighbors = n_neighbors - neighbors_sum\n",
    "\n",
    "    # Majority threshold: strict majority of neighbors being False\n",
    "    majority_threshold = n_neighbors // 2\n",
    "\n",
    "    # New mask: keep original value unless majority of neighbors are False -> set False\n",
    "    new_mask = mask.copy().astype(bool)\n",
    "    majority_false = false_neighbors > majority_threshold\n",
    "    new_mask[majority_false] = False\n",
    "\n",
    "    return new_mask\n",
    "\n",
    "def select_regions_with_high_iou(A: np.ndarray, B: np.ndarray, iou_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Select regions in mask A that have IOU above threshold with any region in mask B.\n",
    "    Returns a boolean mask of selected regions in A.\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import label\n",
    "\n",
    "    # Label connected components in A and B\n",
    "    labeled_A, num_A = label(A)\n",
    "    labeled_B, num_B = label(B)\n",
    "\n",
    "    selected_mask = np.zeros_like(A, dtype=bool)\n",
    "\n",
    "    # For each region in A\n",
    "    for a_idx in range(1, num_A + 1):\n",
    "        region_A = labeled_A == a_idx\n",
    "        max_iou = 0\n",
    "        # Compare with each region in B\n",
    "        for b_idx in range(1, num_B + 1):\n",
    "            region_B = labeled_B == b_idx\n",
    "            intersection = np.logical_and(region_A, region_B).sum()\n",
    "            union = np.logical_or(region_A, region_B).sum()\n",
    "            if union == 0:\n",
    "                continue\n",
    "            iou = intersection / union\n",
    "            if iou > max_iou:\n",
    "                max_iou = iou\n",
    "        # If max IOU above threshold, select region\n",
    "        if max_iou >= iou_threshold:\n",
    "            selected_mask = np.logical_or(selected_mask, region_A)\n",
    "\n",
    "    return selected_mask\n",
    "\n",
    "def process_mask(row):\n",
    "    \n",
    "    mask_name = row[\"name\"]\n",
    "    \n",
    "    # print(mask_name, row[\"index\"])\n",
    "    \n",
    "    forged_image_path = os.path.join(forged_folder, mask_name+'.png')\n",
    "    forged_img = Image.open(forged_image_path).convert('RGB')\n",
    "\n",
    "    auth_image_path = os.path.join(authentic_folder, mask_name+'.png')\n",
    "    auth_img = Image.open(auth_image_path).convert('RGB')\n",
    "    \n",
    "    # Subtract auth_img from forged_img and convert to boolean mask\n",
    "    forged_array = np.mean(np.array(forged_img), axis=2)\n",
    "    # forged_array = (forged_array - np.min(forged_array))/(np.max(forged_array) - np.min(forged_array))\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(forged_array, cmap='gray')\n",
    "    # plt.title(f'Forged')\n",
    "    # plt.axis('off')\n",
    "    \n",
    "    auth_array = np.mean(np.array(auth_img), axis=2)\n",
    "    # auth_array = (auth_array - np.min(auth_array))/(np.max(auth_array) - np.min(auth_array))\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(auth_array, cmap='gray')\n",
    "    # plt.title(f'Auth')\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # Calculate absolute difference\n",
    "    diff_array = np.abs(forged_array.astype(np.float32) - auth_array.astype(np.float32))\n",
    "\n",
    "    # Convert to grayscale by taking mean across channels\n",
    "    # diff_gray = np.mean(diff_array, axis=2)\n",
    "\n",
    "    # Create boolean mask (threshold can be adjusted)\n",
    "    DIFF_THRESHOLD = 0  # Adjust this value as needed\n",
    "    diff_boolean_mask = diff_array > DIFF_THRESHOLD\n",
    "    diff_boolean_mask = diff_boolean_mask.astype(bool)\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(diff_boolean_mask, cmap='gray')\n",
    "    # plt.title(f'Diff')\n",
    "    # plt.axis('off')\n",
    "    \n",
    "    \n",
    "    if not np.any(diff_boolean_mask):\n",
    "        return None\n",
    "    \n",
    "    # Smooth the diff boolean mask\n",
    "    kernel_size = 3\n",
    "    last_diff_boolean_mask = diff_boolean_mask.copy()\n",
    "    it = 0\n",
    "    while 1:\n",
    "        while 1:\n",
    "            it += 1\n",
    "            prev_mask = diff_boolean_mask.copy()\n",
    "            diff_boolean_mask = smooth_boolean_mask2(diff_boolean_mask, kernel_size=kernel_size)\n",
    "            if np.array_equal(prev_mask, diff_boolean_mask):\n",
    "                del prev_mask\n",
    "                gc.collect()\n",
    "                break\n",
    "        \n",
    "        n_comps = count_components_floodfill({\"name\":mask_name, \\\n",
    "                                    \"index\":-1, \\\n",
    "                                    \"bool_mask\":diff_boolean_mask})[\"n_comps\"]\n",
    "        kernel_size += 2\n",
    "        if not np.any(diff_boolean_mask):\n",
    "            break\n",
    "        if np.array_equal(last_diff_boolean_mask, diff_boolean_mask):\n",
    "            continue\n",
    "        last_diff_boolean_mask = diff_boolean_mask\n",
    "        if n_comps <= row[\"n_comps\"]-1:\n",
    "            break\n",
    "        \n",
    "    diff_boolean_mask = last_diff_boolean_mask\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(diff_boolean_mask, cmap='gray')\n",
    "    # plt.title(f'Diff')\n",
    "    # plt.axis('off')\n",
    "    \n",
    "    \n",
    "    masked_img_boolean = row[\"bool_mask\"]\n",
    "    # masked_img_boolean = resize_mask_to_patch_dimensions(masked_img_boolean)\n",
    "    \n",
    "    #  # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(masked_img_boolean, cmap='gray')\n",
    "    # plt.title(f'OG Mask')\n",
    "    # plt.axis('off')\n",
    "    \n",
    "    \n",
    "    # find intersections between diff_boolean_mask and masked_img_boolean\n",
    "    forged_mask = np.logical_and(diff_boolean_mask, masked_img_boolean)\n",
    "    if not np.any(diff_boolean_mask):\n",
    "        return None\n",
    "    \n",
    "    # Smooth the boolean mask\n",
    "    kernel_size = 3\n",
    "    last_forged_mask = forged_mask.copy()\n",
    "    it = 0\n",
    "    while 1:\n",
    "        while 1:\n",
    "            it += 1\n",
    "            prev_mask = forged_mask.copy()\n",
    "            forged_mask = smooth_boolean_mask2(forged_mask, kernel_size=kernel_size)\n",
    "            if np.array_equal(prev_mask, forged_mask):\n",
    "                del prev_mask\n",
    "                gc.collect()\n",
    "                break\n",
    "        n_comps = count_components_floodfill({\"name\":mask_name, \\\n",
    "                                    \"index\":-1, \\\n",
    "                                    \"bool_mask\":forged_mask})[\"n_comps\"]\n",
    "        kernel_size += 2\n",
    "        if not np.any(forged_mask):\n",
    "            break\n",
    "        if np.array_equal(last_forged_mask, forged_mask):\n",
    "            continue\n",
    "        last_forged_mask = forged_mask\n",
    "        if n_comps <= row[\"n_comps\"]-1:\n",
    "            break\n",
    "    forged_mask = last_forged_mask\n",
    "    \n",
    "    # Select the regions with highest IOU\n",
    "    forged_mask = select_regions_with_high_iou(A=masked_img_boolean, B=forged_mask, iou_threshold=0.85)\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(forged_mask, cmap='gray')\n",
    "    # plt.title(f'Forged')\n",
    "    # plt.axis('off')\n",
    "    \n",
    "    # get auth mask\n",
    "    auth_mask = masked_img_boolean & ~forged_mask\n",
    "    \n",
    "    # # visualize the masks\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.imshow(auth_mask, cmap='gray')\n",
    "    # plt.title(f'Auth')\n",
    "    # plt.axis('off')\n",
    "\n",
    "\n",
    "    me = {\"mask_name\": mask_name, \\\n",
    "                \"index\": row[\"index\"], \\\n",
    "                \"auth_mask\": auth_mask, \\\n",
    "                \"forged_mask\": forged_mask,}\n",
    "    \n",
    "    pickle.dump(me, open(os.path.join('mask_entities', f'{mask_name}_{row[\"index\"]}.pkl'), 'wb'))\n",
    "\n",
    "component_counts = pickle.load(open('component_counts.pkl', 'rb'))\n",
    "\n",
    "# chunk_size = 24  # adjust as needed\n",
    "# total = len(component_counts)\n",
    "# n_chunks = (total + chunk_size - 1) // chunk_size\n",
    "# for chunk_idx, start in enumerate(range(0, total, chunk_size), 1):\n",
    "#     end = min(start + chunk_size, total)\n",
    "#     chunk = component_counts[start:end]\n",
    "#     print(f\"Processing chunk {chunk_idx}/{n_chunks} (items {start}:{end})\")\n",
    "#     # process this chunk in parallel\n",
    "#     Parallel(n_jobs=-1)(delayed(process_mask)(row) for row in tqdm(chunk))\n",
    "#     # masked_entities = [item for sublist in masked_entities for item in sublist]\n",
    "#     # pickle.dump(masked_entities, open(os.path.join('mask_entities', f'mask_components{chunk_idx}.pkl'), 'wb'))\n",
    "    \n",
    "\n",
    "\n",
    "# for row in tqdm(component_counts):\n",
    "#     process_mask(row)\n",
    "\n",
    "# masked_entities = Parallel(n_jobs=-1)(delayed(process_mask)(row) for row in tqdm(component_counts[:36]))\n",
    "# masked_entities = [process_mask(component_counts[0])] # 19684  13511\n",
    "[process_mask(x) for x in component_counts if x[\"name\"]==\"10176\" and x[\"index\"]==0]\n",
    "# masked_entities = [item for item in masked_entities if item is not None]\n",
    "# pickle.dump(masked_entities, open('mask_components.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7ab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "masked_entities = load_10_mask_entities(name=\"1143\", idxs=[0,1,2])\n",
    "print(masked_entities[0])\n",
    "print(len(masked_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "\n",
    "print(count_components_floodfill({\"name\":masked_entities[idx][\"mask_name\"], \"index\":masked_entities[idx][\"index\"], \"bool_mask\":masked_entities[idx][\"auth_mask\"]}))\n",
    "print(masked_entities[idx][\"auth_mask\"].shape)\n",
    "\n",
    "# visualize the masks\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(masked_entities[idx][\"auth_mask\"], cmap='gray')\n",
    "plt.title(f'Auth')\n",
    "plt.axis('off')\n",
    "\n",
    "# visualize the masks\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(masked_entities[idx][\"forged_mask\"], cmap='gray')\n",
    "plt.title(f'Forged')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73080094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def auth_and_forged_sim_mat(auth_mask, forged_mask):\n",
    "#     \"\"\"\n",
    "#     Calculate similarity matrix between authentic and forged patches using matrix multiplication\n",
    "    \n",
    "#     Args:\n",
    "#         auth_mask: Boolean array of shape [H1, W1]\n",
    "#         forged_mask: Boolean array of shape [H2, W2]\n",
    "        \n",
    "#     Returns:\n",
    "#         similarity_matrix: Tensor of shape [H1, W1, H2, W2]\n",
    "#     \"\"\"\n",
    "#     # Convert boolean masks to float tensors\n",
    "#     if not isinstance(auth_mask, torch.Tensor):\n",
    "#         auth_mask = torch.from_numpy(auth_mask.astype(np.float32))\n",
    "#     if not isinstance(forged_mask, torch.Tensor):\n",
    "#         forged_mask = torch.from_numpy(forged_mask.astype(np.float32))\n",
    "    \n",
    "#     # Get dimensions\n",
    "#     H1, W1 = auth_mask.shape\n",
    "#     H2, W2 = forged_mask.shape\n",
    "    \n",
    "#     # Reshape to [N1, 1] and [N2, 1] where N1 = H1*W1 and N2 = H2*W2\n",
    "#     auth_features = auth_mask.reshape(-1, 1)\n",
    "#     forged_features = forged_mask.reshape(-1, 1)\n",
    "    \n",
    "#     # Normalize features\n",
    "#     auth_features = F.normalize(auth_features, p=2, dim=1)\n",
    "#     forged_features = F.normalize(forged_features, p=2, dim=1)\n",
    "    \n",
    "#     # Calculate similarity matrix: [N1, 1] Ã— [1, N2] = [N1, N2]\n",
    "#     similarity_matrix = torch.matmul(auth_features, forged_features.T)\n",
    "    \n",
    "#     # Reshape to [H1, W1, H2, W2] for easier interpretation\n",
    "#     similarity_matrix = similarity_matrix.reshape(H1, W1, H2, W2)\n",
    "    \n",
    "#     return similarity_matrix\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# def visualize_similarity_matrix(similarity_matrix, title=\"Patch Similarity Matrix\"):\n",
    "#     \"\"\"Visualize the similarity matrix with a heatmap\"\"\"\n",
    "#     H1, W1, H2, W2 = similarity_matrix.shape\n",
    "    \n",
    "#     # Reshape to 2D for visualization\n",
    "#     sim_mat_2d = similarity_matrix.reshape(H1*W1, H2*W2)\n",
    "    \n",
    "#     plt.figure(figsize=(100, 80))\n",
    "#     plt.imshow(sim_mat_2d.cpu().numpy(), cmap='viridis', vmin=-1, vmax=1)\n",
    "#     plt.colorbar(label='Cosine Similarity')\n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Forged Patches')\n",
    "#     plt.ylabel('Authentic Patches')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Use with your data:\n",
    "    \n",
    "# # Calculate similarity matrix\n",
    "# idx = 29\n",
    "# similarity_matrix = auth_and_forged_sim_mat(\n",
    "#     masked_entities[idx][\"auth_mask\"],\n",
    "#     masked_entities[idx][\"forged_mask\"]\n",
    "# )\n",
    "\n",
    "# # Visualize\n",
    "# visualize_similarity_matrix(\n",
    "#     similarity_matrix,\n",
    "#     f\"Patch Similarity Matrix for {masked_entities[0]['mask_name']}\"\n",
    "# )\n",
    "\n",
    "# # Optional: Find highest similarity matches\n",
    "# max_similarities, max_indices = torch.max(similarity_matrix.reshape(-1, similarity_matrix.shape[-1]), dim=1)\n",
    "\n",
    "# print(f\"Top 5 highest similarities:\")\n",
    "# top_k = 5\n",
    "# top_similarities, top_indices = torch.topk(max_similarities, top_k)\n",
    "\n",
    "# for i, (sim, idx) in enumerate(zip(top_similarities, top_indices)):\n",
    "#     auth_idx = idx // similarity_matrix.shape[-1]\n",
    "#     forged_idx = idx % similarity_matrix.shape[-1]\n",
    "#     print(f\"{i+1}. Similarity: {sim:.3f}, Auth patch: {auth_idx}, Forged patch: {forged_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "row = masked_entities[idx]\n",
    "\n",
    "forged_image_path = os.path.join(forged_folder, row[\"mask_name\"]+'.png')\n",
    "forged_image = Image.open(forged_image_path).convert('RGB')\n",
    "\n",
    "print(resize_image_to_fit_patch(forged_image).size)\n",
    "# Forged\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(resize_image_to_fit_patch(forged_image))\n",
    "plt.title(f'Forged')\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "auth_image_path = os.path.join(authentic_folder, row[\"mask_name\"]+'.png')\n",
    "auth_image = Image.open(auth_image_path).convert('RGB')\n",
    "\n",
    "# forged\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(resize_image_to_fit_patch(auth_image))\n",
    "plt.title(f'Auth')\n",
    "plt.axis('off')\n",
    "\n",
    "print(resize_mask_to_fit_patch(row[\"auth_mask\"]).shape)\n",
    "# visualize the masks\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(resize_mask_to_fit_patch(row[\"auth_mask\"]), cmap='gray')\n",
    "plt.title(f'Auth')\n",
    "plt.axis('off')\n",
    "\n",
    "# visualize the masks\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(resize_mask_to_fit_patch(row[\"forged_mask\"]), cmap='gray')\n",
    "plt.title(f'Forged')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = MODEL_DINOV3_VITS\n",
    "N_LAYERS = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
    "EMBED_DIM = MODEL_TO_EMBED_DIM[MODEL_NAME]\n",
    "WEIGHT_FILE = MODEL_TO_WEIGHT_FILE[MODEL_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8d9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv7 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "        self.conv8 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(hidden_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        \n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        \n",
    "        x = F.relu(self.bn6(self.conv6(x)))\n",
    "        \n",
    "        x = F.relu(self.bn7(self.conv7(x)))\n",
    "        \n",
    "        x = F.relu(self.bn8(self.conv8(x)))\n",
    "        return x\n",
    "\n",
    "class DINOv3Segmentation(nn.Module):\n",
    "    def __init__(self, backbone, in_channels=EMBED_DIM):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.seg_head = SegmentationHead(in_channels)\n",
    "        \n",
    "        # Freeze backbone\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Initialize segmentation head\n",
    "        for m in self.seg_head.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get features from backbone\n",
    "        features = self.backbone.get_intermediate_layers(\n",
    "            x,\n",
    "            n=range(N_LAYERS),\n",
    "            reshape=True,\n",
    "            norm=True\n",
    "        )\n",
    "        last_layer_features = features[-1]  # Shape: [B, C, H, W]\n",
    "        \n",
    "        # Pass through segmentation head\n",
    "        last_layer_features = self.seg_head(last_layer_features)\n",
    "        return last_layer_features.permute(0, 2, 3, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2467ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6c881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for img_batch, all_sims, _ in tqdm(get_batches()):\n",
    "    \n",
    "    with torch.inference_mode(): \n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
    "            \n",
    "            layer_feats = model.get_intermediate_layers(img_batch.cuda(), \\\n",
    "                                                    n=range(n_layers), \\\n",
    "                                                    reshape=True, \\\n",
    "                                                    norm=True)\n",
    "            \n",
    "            patch_features = layer_feats[-1].detach().cpu()\n",
    "            \n",
    "            # Convert from [B, D, H, W] to [B, H, W, D]\n",
    "            patch_features = patch_features.permute(0, 2, 3, 1)\n",
    "            print(patch_features.shape)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50980d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segmentation model\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=dinov3_repo_dir,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\",\n",
    "    weights=WEIGHT_FILE,\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f9287b-94de-44d0-8346-5fc5232f12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for img_batch, all_sims, ids in tqdm(get_batches(batch_size=16)):\n",
    "    \n",
    "    with torch.inference_mode(): \n",
    "            \n",
    "        layer_feats = model.get_intermediate_layers(img_batch.cpu(), \\\n",
    "                                                n=range(N_LAYERS), \\\n",
    "                                                reshape=True, \\\n",
    "                                                norm=True)\n",
    "        \n",
    "        layer_feats = layer_feats[-1]\n",
    "        \n",
    "        # Convert from [B, D, H, W] to [B, H, W, D]\n",
    "        layer_feats = layer_feats.permute(0, 2, 3, 1)\n",
    "        print(layer_feats.shape)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa1b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading ---\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    # Recreate model architecture\n",
    "    model = torch.hub.load(\n",
    "        repo_or_dir=dinov3_repo_dir,\n",
    "        model=checkpoint['model_name'],\n",
    "        source=\"local\",\n",
    "        weights=dinov3_vits16_weight_raw,\n",
    "    )\n",
    "    seg_model = DINOv3Segmentation(model, in_channels=384).cpu()\n",
    "    seg_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer = torch.optim.AdamW(seg_model.seg_head.parameters())\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return seg_model, optimizer\n",
    "\n",
    "# Example usage:\n",
    "MODEL_NAME = 'dinov3_vits16'\n",
    "checkpoint_path = os.path.join('weights', f'{MODEL_NAME}_dinov3_peft.pth')\n",
    "seg_model, optimizer = load_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c5e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_batch, all_sims, ids in tqdm(get_batches(batch_size=16)):\n",
    "    \n",
    "    with torch.inference_mode(): \n",
    "        with torch.autocast(device_type='cpu', dtype=torch.float32):\n",
    "            \n",
    "            print(img_batch.shape)\n",
    "            layer_feats = seg_model(img_batch.cpu())\n",
    "            print(layer_feats.shape)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c828633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_features_cosine(feats, n_clusters=3, visualize=True):\n",
    "    \"\"\"\n",
    "    Cluster [H, W, D] features using KMeans on cosine-normalized D vectors (no spatial locality).\n",
    "\n",
    "    Args:\n",
    "        feats (torch.Tensor): Feature tensor of shape [H, W, D]\n",
    "        n_clusters (int): Number of clusters\n",
    "        visualize (bool): If True, show cluster map\n",
    "\n",
    "    Returns:\n",
    "        cluster_map (np.ndarray): Cluster labels of shape [H, W]\n",
    "    \"\"\"\n",
    "    H, W, D = feats.shape\n",
    "    feats_flat = feats.view(-1, D)  # [H*W, D]\n",
    "    feats_norm = F.normalize(feats_flat, dim=1).cpu().numpy()  # Normalize for cosine similarity\n",
    "\n",
    "    # KMeans clustering (no spatial locality, only feature similarity)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init='auto')\n",
    "    labels = kmeans.fit_predict(feats_norm)  # [H*W]\n",
    "    cluster_map = labels.reshape(H, W)\n",
    "\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cluster_map, cmap='tab10')\n",
    "        plt.title(f'Cluster Map (KMeans, n={n_clusters})')\n",
    "        plt.axis('off')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    return cluster_map\n",
    "\n",
    "def get_segmentation_map_from_features(feats, threshold=0.8, visualize=True):\n",
    "    \"\"\"\n",
    "    Given a tensor of shape [H, W, D], compute a segmentation map based on cosine similarity\n",
    "    to the most common feature vector (mode).\n",
    "    \"\"\"\n",
    "    H, W, D = feats.shape\n",
    "    feats_flat = feats.view(-1, D)  # [H*W, D]\n",
    "\n",
    "    # # --- CHANGE START ---\n",
    "    # # Find the most common vector (mode)\n",
    "    # # Note: For raw floating point features, you might want to round them first \n",
    "    # # to increase overlaps, e.g.: rounded_feats = torch.round(feats_flat, decimals=2)\n",
    "    # rounded_feats =  torch.round(feats_flat, decimals=0)\n",
    "    # unique_vectors, counts = torch.unique(rounded_feats, return_counts=True, dim=0)\n",
    "    # # unique_vectors, counts = torch.unique(feats_flat, return_counts=True, dim=0)\n",
    "    # most_common_idx = torch.argmax(counts)\n",
    "    # ref_vec = unique_vectors[most_common_idx].unsqueeze(0)  # [1, D]\n",
    "    # # --- CHANGE END ---\n",
    "    \n",
    "    ref_vec = torch.mean(feats_flat, dim=0, keepdim=True)  # [1, D]\n",
    "\n",
    "    feats_norm = F.normalize(feats_flat, dim=1)\n",
    "    ref_norm = F.normalize(ref_vec, dim=1)\n",
    "    cos_sim = torch.matmul(feats_norm, ref_norm.t()).squeeze()  # [H*W]\n",
    "    cos_sim_map = cos_sim.view(H, W)\n",
    "    segmentation_map = (cos_sim_map < (1-threshold)).float()\n",
    "\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cos_sim_map.cpu().numpy(), cmap='viridis')\n",
    "        plt.colorbar(label='Cosine Similarity')\n",
    "        plt.title('Cosine Similarity Heatmap (Ref: Most Common)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(segmentation_map.cpu().numpy(), cmap='gray')\n",
    "        plt.title('Segmentation Map (Thresholded)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return segmentation_map, cos_sim_map\n",
    "\n",
    "idx = 10\n",
    "\n",
    "# Forged\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_batch[idx].squeeze(0).permute(1,2,0))\n",
    "plt.title(f'IMG Forged')\n",
    "plt.axis('off')\n",
    "\n",
    "idx_files = glob(os.path.join(\"mask_entities\", f'{ids[idx]}_*.pkl'))\n",
    "for file in idx_files:\n",
    "    me = pickle.load(open(file, 'rb'))\n",
    "    \n",
    "    auth_mask = resize_mask_to_fit_patch(me[\"auth_mask\"])\n",
    "    # auth_mask = pixel_mask_to_patch_float(auth_mask)\n",
    "    \n",
    "    forged_mask = resize_mask_to_fit_patch(me[\"forged_mask\"])\n",
    "    # forged_mask = pixel_mask_to_patch_float(forged_mask)\n",
    "    \n",
    "    # visualize the masks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(auth_mask, cmap='gray')\n",
    "    plt.title(f'Auth Mask')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # visualize the masks\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(forged_mask, cmap='gray')\n",
    "    plt.title(f'Forged Mask')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Example usage:\n",
    "seg_map, sim_map = get_segmentation_map_from_features(layer_feats[idx], threshold=0.95)\n",
    "\n",
    "cluster_features_cosine(layer_feats[idx], n_clusters=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29df8d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c57cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe967db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef33d326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68235298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numba\n",
    "import numpy as np\n",
    "from numba import types\n",
    "import numpy.typing as npt\n",
    "import pandas as pd\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def _rle_encode_jit(x: npt.NDArray, fg_val: int = 1) -> list[int]:\n",
    "    \"\"\"Numba-jitted RLE encoder.\"\"\"\n",
    "    dots = np.where(x.T.flatten() == fg_val)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if b > prev + 1:\n",
    "            run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "\n",
    "def rle_encode(masks: list[npt.NDArray], fg_val: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    Adapted from contrails RLE https://www.kaggle.com/code/inversion/contrails-rle-submission\n",
    "    Args:\n",
    "        masks: list of numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns: run length encodings as a string, with each RLE JSON-encoded and separated by a semicolon.\n",
    "    \"\"\"\n",
    "    return ';'.join([json.dumps(_rle_encode_jit(x, fg_val)) for x in masks])\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def _rle_decode_jit(mask_rle: npt.NDArray, height: int, width: int) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    s: numpy array of run-length encoding pairs (start, length)\n",
    "    shape: (height, width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \"\"\"\n",
    "    if len(mask_rle) % 2 != 0:\n",
    "        # Numba requires raising a standard exception.\n",
    "        raise ValueError('One or more rows has an odd number of values.')\n",
    "\n",
    "    starts, lengths = mask_rle[0::2], mask_rle[1::2]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    for i in range(len(starts) - 1):\n",
    "        if ends[i] > starts[i + 1]:\n",
    "            raise ValueError('Pixels must not be overlapping.')\n",
    "    img = np.zeros(height * width, dtype=np.bool_)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img\n",
    "\n",
    "\n",
    "def rle_decode(mask_rle: str, shape: tuple[int, int]) -> npt.NDArray:\n",
    "    \"\"\"\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "              empty predictions need to be encoded with '-'\n",
    "    shape: (height, width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    \"\"\"\n",
    "\n",
    "    mask_rle = json.loads(mask_rle)\n",
    "    mask_rle = np.asarray(mask_rle, dtype=np.int32)\n",
    "    starts = mask_rle[0::2]\n",
    "    if sorted(starts) != list(starts):\n",
    "        raise ParticipantVisibleError('Submitted values must be in ascending order.')\n",
    "    try:\n",
    "        return _rle_decode_jit(mask_rle, shape[0], shape[1]).reshape(shape, order='F')\n",
    "    except ValueError as e:\n",
    "        raise ParticipantVisibleError(str(e)) from e\n",
    "\n",
    "\n",
    "def calculate_f1_score(pred_mask: npt.NDArray, gt_mask: npt.NDArray):\n",
    "    pred_flat = pred_mask.flatten()\n",
    "    gt_flat = gt_mask.flatten()\n",
    "\n",
    "    tp = np.sum((pred_flat == 1) & (gt_flat == 1))\n",
    "    fp = np.sum((pred_flat == 1) & (gt_flat == 0))\n",
    "    fn = np.sum((pred_flat == 0) & (gt_flat == 1))\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    if (precision + recall) > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def calculate_f1_matrix(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    pred_masks (np.ndarray):\n",
    "            First dimension is the number of predicted instances.\n",
    "            Each instance is a binary mask of shape (height, width).\n",
    "    gt_masks (np.ndarray):\n",
    "            First dimension is the number of ground truth instances.\n",
    "            Each instance is a binary mask of shape (height, width).\n",
    "    \"\"\"\n",
    "\n",
    "    num_instances_pred = len(pred_masks)\n",
    "    num_instances_gt = len(gt_masks)\n",
    "    f1_matrix = np.zeros((num_instances_pred, num_instances_gt))\n",
    "\n",
    "    # Calculate F1 scores for each pair of predicted and ground truth masks\n",
    "    for i in range(num_instances_pred):\n",
    "        for j in range(num_instances_gt):\n",
    "            pred_flat = pred_masks[i].flatten()\n",
    "            gt_flat = gt_masks[j].flatten()\n",
    "            f1_matrix[i, j] = calculate_f1_score(pred_mask=pred_flat, gt_mask=gt_flat)\n",
    "\n",
    "    if f1_matrix.shape[0] < len(gt_masks):\n",
    "        # Add a row of zeros to the matrix if the number of predicted instances is less than ground truth instances\n",
    "        f1_matrix = np.vstack((f1_matrix, np.zeros((len(gt_masks) - len(f1_matrix), num_instances_gt))))\n",
    "\n",
    "    return f1_matrix\n",
    "\n",
    "\n",
    "def oF1_score(pred_masks: list[npt.NDArray], gt_masks: list[npt.NDArray]):\n",
    "    \"\"\"\n",
    "    Calculate the optimal F1 score for a set of predicted masks against\n",
    "    ground truth masks which considers the optimal F1 score matching.\n",
    "    This function uses the Hungarian algorithm to find the optimal assignment\n",
    "    of predicted masks to ground truth masks based on the F1 score matrix.\n",
    "    If the number of predicted masks is less than the number of ground truth masks,\n",
    "    it will add a row of zeros to the F1 score matrix to ensure that the dimensions match.\n",
    "\n",
    "    Parameters:\n",
    "    pred_masks (list of np.ndarray): List of predicted binary masks.\n",
    "    gt_masks (np.ndarray): Array of ground truth binary masks.\n",
    "    Returns:\n",
    "    float: Optimal F1 score.\n",
    "    \"\"\"\n",
    "    f1_matrix = calculate_f1_matrix(pred_masks, gt_masks)\n",
    "\n",
    "    # Find the best matching between predicted and ground truth masks\n",
    "    row_ind, col_ind = scipy.optimize.linear_sum_assignment(-f1_matrix)\n",
    "    # The linear_sum_assignment discards excess predictions so we need a separate penalty.\n",
    "    excess_predictions_penalty = len(gt_masks) / max(len(pred_masks), len(gt_masks))\n",
    "    return np.mean(f1_matrix[row_ind, col_ind]) * excess_predictions_penalty\n",
    "\n",
    "\n",
    "def evaluate_single_image(label_rles: str, prediction_rles: str, shape_str: str) -> float:\n",
    "    shape = json.loads(shape_str)\n",
    "    label_rles = [rle_decode(x, shape=shape) for x in label_rles.split(';')]\n",
    "    prediction_rles = [rle_decode(x, shape=shape) for x in prediction_rles.split(';')]\n",
    "    return oF1_score(prediction_rles, label_rles)\n",
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        solution (pd.DataFrame): The ground truth DataFrame.\n",
    "        submission (pd.DataFrame): The submission DataFrame.\n",
    "        row_id_column_name (str): The name of the column containing row IDs.\n",
    "    Returns:\n",
    "        float\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['authentic', 'authentic', 'authentic'], 'shape': ['authentic', 'authentic', 'authentic']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    1.0\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 103]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.9983739837398374\n",
    "\n",
    "    >>> solution = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102];[300, 100]', '[101, 102]', '[101, 102]'], 'shape': ['[720, 960]', '[720, 960]', '[720, 960]']})\n",
    "    >>> submission = pd.DataFrame({'row_id': [0, 1, 2], 'annotation': ['[101, 102]', '[101, 102]', '[101, 102]']})\n",
    "    >>> score(solution.copy(), submission.copy(), row_id_column_name='row_id')\n",
    "    0.8333333333333334\n",
    "    \"\"\"\n",
    "    df = solution\n",
    "    df = df.rename(columns={'annotation': 'label'})\n",
    "\n",
    "    df['prediction'] = submission['annotation']\n",
    "    # Check for correct 'authentic' label\n",
    "    authentic_indices = (df['label'] == 'authentic') | (df['prediction'] == 'authentic')\n",
    "    df['image_score'] = ((df['label'] == df['prediction']) & authentic_indices).astype(float)\n",
    "\n",
    "    df.loc[~authentic_indices, 'image_score'] = df.loc[~authentic_indices].apply(\n",
    "        lambda row: evaluate_single_image(row['label'], row['prediction'], row['shape']), axis=1\n",
    "    )\n",
    "    return float(np.mean(df['image_score']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b836125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25a248-b801-4af6-b1bb-f1040288f1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8fa520-ae39-42d7-b964-6bee79cca4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rle_encode([seg_map.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06daa42-0a1a-4e3f-b09b-5b078e4828d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25186f-e6e1-4fad-8f72-43b5ce869c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_map > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb114c1-22ee-4949-83df-56c785e2986d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d005821-a790-48ce-b570-37dd9bfc9c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdff7050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
